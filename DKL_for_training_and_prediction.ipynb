{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef27c2b9-513d-4e05-a281-2ddd26052454",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import gpytorch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from gpytorch.models import ApproximateGP\n",
    "from gpytorch.variational import BatchDecoupledVariationalStrategy\n",
    "from gpytorch.variational import CholeskyVariationalDistribution\n",
    "from gpytorch.likelihoods import GaussianLikelihood\n",
    "from gpytorch.mlls import PredictiveLogLikelihood\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.utils.data import DataLoader, TensorDataset, Subset\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, f1_score,recall_score,precision_score\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import KFold\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c4379b2-4ecb-4706-ab28-4ef8fb67bc4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"path to save the trained models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "275f5688-f956-43cb-872d-b9fe31fcd882",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('path to training dataset')\n",
    "# Assume the last column is the target and the rest are features\n",
    "X = df.iloc[:, 4:-1]\n",
    "y = df.iloc[:, -1].values\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "# Save the fitted scaler for later use during prediction\n",
    "joblib.dump(scaler, 'scaler.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ad1e3d-c94d-40f6-b985-63b63273aef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1, use_instance_norm=False):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels, out_channels, kernel_size=3, padding=1, stride=stride)\n",
    "        self.bn1 = nn.InstanceNorm1d(out_channels) if use_instance_norm else nn.BatchNorm1d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        \n",
    "        self.conv2 = nn.Conv1d(out_channels, out_channels, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.InstanceNorm1d(out_channels) if use_instance_norm else nn.BatchNorm1d(out_channels)\n",
    "        \n",
    "        self.downsample = None\n",
    "        if stride != 1 or in_channels != out_channels:\n",
    "            self.downsample = nn.Sequential(\n",
    "                nn.Conv1d(in_channels, out_channels, kernel_size=1, stride=stride),\n",
    "                nn.InstanceNorm1d(out_channels) if use_instance_norm else nn.BatchNorm1d(out_channels),\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "        return out\n",
    "\n",
    "class ResNet1D(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ResNet1D, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(1, 40, kernel_size=7, stride=2, padding=3)  # Initial convolution\n",
    "        self.bn1 = nn.BatchNorm1d(40)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool1d(kernel_size=3, stride=2, padding=1)\n",
    "        \n",
    "        self.layer1 = ResidualBlock(40, 40)\n",
    "        self.layer2 = ResidualBlock(40, 64, stride=2)\n",
    "        self.layer3 = ResidualBlock(64, 128, stride=2)\n",
    "        self.layer4 = ResidualBlock(128, 256, stride=2)\n",
    "        \n",
    "        \n",
    "        # Adding Dropout here \n",
    "        self.dropout = nn.Dropout(0.5)  \n",
    "        \n",
    "        self.global_pool = nn.AdaptiveAvgPool1d(1)\n",
    "        self.fc = nn.Linear(256, 1)  # first full-connected\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "\n",
    "        # Apply dropout after Layer 2 and before global pooling\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.global_pool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "783545bd-7154-47b2-930f-d6d51b931376",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNetGP(nn.Module):\n",
    "    def __init__(self, feature_extractor):\n",
    "        super(ResNetGP, self).__init__()\n",
    "        self.feature_extractor = feature_extractor\n",
    "        # Initialize GP Model assuming the last layer of the feature extractor is fully connected\n",
    "        self.gp = GPModel(self.feature_extractor.fc.out_features)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # First, pass inputs through the feature extractor\n",
    "        features = self.feature_extractor(x)\n",
    "        # Ensure features are flattened if not already (important if coming from CNNs)\n",
    "        if features.dim() > 2:\n",
    "            features = features.view(features.size(0), -1)\n",
    "        # Then pass the features through the GP model\n",
    "        output = self.gp(features)\n",
    "        return output\n",
    "\n",
    "class GPModel(ApproximateGP):\n",
    "    def __init__(self, feature_dim):\n",
    "        variational_distribution = CholeskyVariationalDistribution(512)  # Number of inducing points\n",
    "        variational_strategy = BatchDecoupledVariationalStrategy(\n",
    "            self, torch.zeros(512, feature_dim), variational_distribution, learn_inducing_locations=True\n",
    "        )\n",
    "        super(GPModel, self).__init__(variational_strategy)\n",
    "        self.mean_module = gpytorch.means.ConstantMean()\n",
    "        self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel())\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "\n",
    "# Assuming ResNet1D is properly defined\n",
    "feature_extractor = ResNet1D()\n",
    "model = ResNetGP(feature_extractor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbbfa40b-0900-4dd4-a70f-ff7907a6f9b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for PyTorch\n",
    "X_k = torch.tensor(X_scaled, dtype=torch.float32).unsqueeze(1)  # Add channel dimension\n",
    "y_k = torch.tensor(y, dtype=torch.long)\n",
    "dataset = TensorDataset(X_k, y_k)\n",
    "\n",
    "def train_and_evaluate(model, train_loader, val_loader, likelihood, optimizer, epochs=30):\n",
    "    # Train the model\n",
    "    model.train()\n",
    "    likelihood.train()\n",
    "    for _ in range(epochs):\n",
    "        for batch_x, batch_y in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            output = model(batch_x)\n",
    "            loss = -model_likelihood(output, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    # Evaluate the model\n",
    "    model.eval()\n",
    "    likelihood.eval()\n",
    "    true_labels = []\n",
    "    pred_probs = []\n",
    "    with torch.no_grad():\n",
    "        for x, y in val_loader:\n",
    "            preds = model(x)\n",
    "            prob_pos = likelihood(preds).mean.detach()\n",
    "            true_labels.extend(y.numpy())\n",
    "            prob_pos = prob_pos.view(-1) \n",
    "            pred_probs.extend(prob_pos.view(-1).tolist())  # Convert tensor to list before extending\n",
    "#            pred_probs.extend(prob_pos.numpy())\n",
    "\n",
    "    accuracy = accuracy_score(true_labels, (np.array(pred_probs) >= 0.7).astype(int))\n",
    "    auc = roc_auc_score(true_labels, pred_probs)\n",
    "    f1 = f1_score(true_labels, (np.array(pred_probs) >= 0.7).astype(int))\n",
    "    recall = recall_score(true_labels, (np.array(pred_probs) >= 0.7).astype(int))\n",
    "    precision = precision_score(true_labels, (np.array(pred_probs) >= 0.7).astype(int))\n",
    "    return accuracy, auc, f1, recall, precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87eced18-9c19-455d-ab75-9f7467cef6f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup K-Fold Cross-Validation\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "results = []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(X_k)):\n",
    "    train_subset = Subset(dataset, train_idx)\n",
    "    val_subset = Subset(dataset, val_idx)\n",
    "    #Stochastic minibatching\n",
    "    train_loader = DataLoader(train_subset, batch_size=128, shuffle=True)\n",
    "    val_loader = DataLoader(val_subset, batch_size=128, shuffle=False)\n",
    "\n",
    "    # Instantiate the model, likelihood, and optimizer\n",
    "    feature_extractor = ResNet1D()  \n",
    "    model = ResNetGP(feature_extractor)\n",
    "    likelihood = GaussianLikelihood()\n",
    "    model_likelihood = PredictiveLogLikelihood(likelihood, model.gp, num_data=len(train_loader.dataset))\n",
    "    optimizer = torch.optim.Adam([\n",
    "        {'params': model.parameters()},\n",
    "        {'params': likelihood.parameters()}\n",
    "    ], lr=0.001,weight_decay=1e-4)\n",
    "\n",
    "    # Train and evaluate the model\n",
    "    accuracy, auc, f1,recall, precision = train_and_evaluate(model, train_loader, val_loader, likelihood, optimizer)\n",
    "    results.append((accuracy, auc, f1,recall, precision))\n",
    "    print(f\"Fold {fold+1}: Accuracy={accuracy:.4f}, AUC={auc:.4f}, F1={f1:.4f},recall={recall:.4f}, precision={precision:.4f}\")\n",
    "\n",
    "    # Save the model and likelihood parameters\n",
    "    torch.save({\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'likelihood_state_dict': likelihood.state_dict()\n",
    "    }, f\"model_and_likelihood_fold_{fold+1}.pth\")\n",
    "\n",
    "# Calculate average metrics across folds\n",
    "average_accuracy = np.mean([res[0] for res in results])\n",
    "average_auc = np.mean([res[1] for res in results])\n",
    "average_f1 = np.mean([res[2] for res in results])\n",
    "average_recall = np.mean([res[3] for res in results])\n",
    "average_precision = np.mean([res[4] for res in results])\n",
    "print(f\"Average Accuracy: {average_accuracy:.4f}, Average AUC: {average_auc:.4f}, Average F1: {average_f1:.4f}, Average recall: {average_recall:.4f}, Average precision: {average_precision:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8668770-c07e-44c1-aa72-5ac80c2d8cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup K-Fold Cross-Validation\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "results = []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(X_k)):\n",
    "    train_subset = Subset(dataset, train_idx)\n",
    "    val_subset = Subset(dataset, val_idx)\n",
    "    #Stochastic minibatching\n",
    "    train_loader = DataLoader(train_subset, batch_size=128, shuffle=True)\n",
    "    val_loader = DataLoader(val_subset, batch_size=128, shuffle=False)\n",
    "\n",
    "    # Instantiate the model, likelihood, and optimizer\n",
    "    feature_extractor = ResNet1D()  \n",
    "    model = ResNetGP(feature_extractor)\n",
    "    likelihood = GaussianLikelihood()\n",
    "    model_likelihood = PredictiveLogLikelihood(likelihood, model.gp, num_data=len(train_loader.dataset))\n",
    "    optimizer = torch.optim.Adam([\n",
    "        {'params': model.parameters()},\n",
    "        {'params': likelihood.parameters()}\n",
    "    ], lr=0.001,weight_decay=1e-4)\n",
    "\n",
    "    # Train and evaluate the model\n",
    "    accuracy, auc, f1,recall, precision = train_and_evaluate(model, train_loader, val_loader, likelihood, optimizer)\n",
    "    results.append((accuracy, auc, f1,recall, precision))\n",
    "    print(f\"Fold {fold+1}: Accuracy={accuracy:.4f}, AUC={auc:.4f}, F1={f1:.4f},recall={recall:.4f}, precision={precision:.4f}\")\n",
    "\n",
    "    # Save the model and likelihood parameters\n",
    "    torch.save({\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'likelihood_state_dict': likelihood.state_dict()\n",
    "    }, f\"model_and_likelihood_fold_{fold+1}.pth\")\n",
    "\n",
    "# Calculate average metrics across folds\n",
    "average_accuracy = np.mean([res[0] for res in results])\n",
    "average_auc = np.mean([res[1] for res in results])\n",
    "average_f1 = np.mean([res[2] for res in results])\n",
    "average_recall = np.mean([res[3] for res in results])\n",
    "average_precision = np.mean([res[4] for res in results])\n",
    "print(f\"Average Accuracy: {average_accuracy:.4f}, Average AUC: {average_auc:.4f}, Average F1: {average_f1:.4f}, Average recall: {average_recall:.4f}, Average precision: {average_precision:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c67134e-85a0-4b5f-8959-86b105f43d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_folder = 'path to load trained models'  # Use raw string for Windows paths\n",
    "\n",
    "def load_model_and_likelihood(model_path):\n",
    "    # Create an instance of the feature extractor\n",
    "    feature_extractor = ResNet1D()\n",
    "    # Create an instance of the ResNetGP model with the feature extractor\n",
    "    model = ResNetGP(feature_extractor)\n",
    "    likelihood = GaussianLikelihood()  # Initialize the likelihood\n",
    "\n",
    "    # Load the saved state dictionaries\n",
    "    checkpoint = torch.load(model_path)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    likelihood.load_state_dict(checkpoint['likelihood_state_dict'])\n",
    "\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    likelihood.eval()  # Set the likelihood to evaluation mode\n",
    "    return model, likelihood  # Return both as a tuple\n",
    "\n",
    "models_and_likelihoods = [load_model_and_likelihood(os.path.join(model_folder, f'model_and_likelihood_fold_{fold}.pth')) for fold in range(1, 6)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e0c9379-6a4a-40f0-96db-415b04a28c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(file_path, scaler_path='scaler.gz'):\n",
    "    data = pd.read_csv(file_path)\n",
    "    features = data.iloc[:, 4:-10].values\n",
    "    scaler = joblib.load(scaler_path)  # Load the pre-fitted scaler\n",
    "    features_normalized = scaler.transform(features)  # Transform, not fit_transform!\n",
    "    \n",
    "    features_tensor = torch.tensor(features_normalized, dtype=torch.float32).unsqueeze(1)\n",
    "    return data, DataLoader(TensorDataset(features_tensor), batch_size=128, shuffle=False)\n",
    "\n",
    "def predict_with_models(models_and_likelihoods, data_loader):\n",
    "    all_preds = []\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            data = batch[0]\n",
    "            # Unpack and use each model and likelihood correctly\n",
    "            probs = [likelihood(model(data)).mean for model, likelihood in models_and_likelihoods]\n",
    "            mean_probs = torch.stack(probs).mean(dim=0)\n",
    "            predicted_classes = (mean_probs >= 0.7).int()\n",
    "            all_preds.extend(predicted_classes.cpu().numpy())\n",
    "    return all_preds\n",
    "\n",
    "def process_and_predict(folder_path, output_folder):\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith('.csv'):\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "            original_data, data_loader = prepare_data(file_path)\n",
    "            predictions = predict_with_models(models_and_likelihoods, data_loader)\n",
    "            original_data['FR'] = predictions  # Append predictions as a new column\n",
    "            # Save updated DataFrame\n",
    "            save_path = os.path.join(output_folder, f'{filename[:-4]}_with_predictions.csv')\n",
    "            original_data.to_csv(save_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0070ecde-d4d0-4f2c-a5de-cead0d9fb613",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_folder = 'path to load historical csv files'\n",
    "output_folder = 'path to save the predict results'\n",
    "process_and_predict(input_folder,output_folder)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
